{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\uddea Prompt Test Runner for Function Calling LLM\n", "\n", "This notebook validates the function call outputs of an LLM (e.g., GPT-4, Azure OpenAI, Microsoft Fabric LLM) against a labeled test set of utterances and expected results."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udce6 Step 1: Load Test Set\n", "import pandas as pd\n", "df = pd.read_csv('prompt_test_set.csv')\n", "df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83e\udde0 Step 2: Define your LLM function calling API (mocked for now)\n", "def call_llm(prompt):\n", "    # Placeholder: Replace with your actual LLM call\n", "    # This could be OpenAI, Azure OpenAI, Fabric, or local model\n", "    return {\n", "        \"function\": \"mock_function\",\n", "        \"parameters\": {\"mock_param\": \"value\"}\n", "    }"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2705 Step 3: Evaluate each prompt\n", "results = []\n", "for _, row in df.iterrows():\n", "    prompt = row['utterance']\n", "    expected_func = row['function']\n", "    expected_params = eval(str(row['parameters']))\n", "    \n", "    response = call_llm(prompt)\n", "    actual_func = response['function']\n", "    actual_params = response['parameters']\n", "    \n", "    result = {\n", "        \"utterance\": prompt,\n", "        \"expected_function\": expected_func,\n", "        \"actual_function\": actual_func,\n", "        \"match_function\": expected_func == actual_func,\n", "        \"expected_parameters\": expected_params,\n", "        \"actual_parameters\": actual_params,\n", "        \"match_parameters\": expected_params == actual_params\n", "    }\n", "    results.append(result)\n", "\n", "df_results = pd.DataFrame(results)\n", "df_results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udcca Step 4: Score the model\n", "accuracy_func = df_results['match_function'].mean()\n", "accuracy_params = df_results['match_parameters'].mean()\n", "print(f\"Function Accuracy: {accuracy_func:.2%}\")\n", "print(f\"Parameter Accuracy: {accuracy_params:.2%}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 5}